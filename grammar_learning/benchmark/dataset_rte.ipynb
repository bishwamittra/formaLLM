{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ec2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/NS/formal-grammar-and-memorization/nobackup/shared/huggingface_cache/hub\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/NS/formal-grammar-and-memorization/nobackup/shared/huggingface_cache/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c8f28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542bb46d8c464bd8a0c3659610cf139a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Load the RTE dataset\n",
    "dataset = load_dataset(\"glue\", \"rte\")\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_validation = dataset[\"validation\"].to_pandas()\n",
    "# df_test = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c08e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Weapons of Mass Destruction Found in Iraq Yet.</td>\n",
       "      <td>Weapons of Mass Destruction Found in Iraq.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A place of sorrow, after Pope John Paul II die...</td>\n",
       "      <td>Pope Benedict XVI is the new leader of the Rom...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Herceptin was already approved to treat the si...</td>\n",
       "      <td>Herceptin can be used to treat breast cancer.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Judie Vivian, chief executive at ProMedica, a ...</td>\n",
       "      <td>The previous name of Ho Chi Minh City was Saigon.</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is due in court later charged with the m...</td>\n",
       "      <td>Paul Stewart Hutchinson is accused of having s...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>There is none. They found as many weapons in t...</td>\n",
       "      <td>Weapons of mass destruction found in Iraq.</td>\n",
       "      <td>1</td>\n",
       "      <td>2485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>Dr. Eric Goosby, a pioneer in the fight agains...</td>\n",
       "      <td>Pepfar is committed to fighting AIDS.</td>\n",
       "      <td>0</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>NASA's Saturn exploration spacecraft, Cassini ...</td>\n",
       "      <td>Titan is the fifteenth of Saturn's known satel...</td>\n",
       "      <td>1</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>Brooklyn Borough Hall featured a Who's Who in ...</td>\n",
       "      <td>The Brooklyn Book Festival is held in Brooklyn...</td>\n",
       "      <td>0</td>\n",
       "      <td>2488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>Turkey is unlikely to become involved in, or a...</td>\n",
       "      <td>U.S. to use Turkish military bases.</td>\n",
       "      <td>1</td>\n",
       "      <td>2489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2490 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence1  \\\n",
       "0     No Weapons of Mass Destruction Found in Iraq Yet.   \n",
       "1     A place of sorrow, after Pope John Paul II die...   \n",
       "2     Herceptin was already approved to treat the si...   \n",
       "3     Judie Vivian, chief executive at ProMedica, a ...   \n",
       "4     A man is due in court later charged with the m...   \n",
       "...                                                 ...   \n",
       "2485  There is none. They found as many weapons in t...   \n",
       "2486  Dr. Eric Goosby, a pioneer in the fight agains...   \n",
       "2487  NASA's Saturn exploration spacecraft, Cassini ...   \n",
       "2488  Brooklyn Borough Hall featured a Who's Who in ...   \n",
       "2489  Turkey is unlikely to become involved in, or a...   \n",
       "\n",
       "                                              sentence2  label   idx  \n",
       "0            Weapons of Mass Destruction Found in Iraq.      1     0  \n",
       "1     Pope Benedict XVI is the new leader of the Rom...      0     1  \n",
       "2         Herceptin can be used to treat breast cancer.      0     2  \n",
       "3     The previous name of Ho Chi Minh City was Saigon.      0     3  \n",
       "4     Paul Stewart Hutchinson is accused of having s...      1     4  \n",
       "...                                                 ...    ...   ...  \n",
       "2485         Weapons of mass destruction found in Iraq.      1  2485  \n",
       "2486              Pepfar is committed to fighting AIDS.      0  2486  \n",
       "2487  Titan is the fifteenth of Saturn's known satel...      1  2487  \n",
       "2488  The Brooklyn Book Festival is held in Brooklyn...      0  2488  \n",
       "2489                U.S. to use Turkish military bases.      1  2489  \n",
       "\n",
       "[2490 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d43ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: \"entailment\",\n",
    "    1: \"contradiction\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae451bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2490, 9), (277, 9))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def process_columns(df):\n",
    "    random.seed(5)\n",
    "    df['label_flipped'] = df['label'].apply(\n",
    "        lambda x: 0 if x == 1 else 1\n",
    "    )\n",
    "\n",
    "    assert (df['label_flipped'] != df['label']).all()\n",
    "    df['label_str'] = df['label'].map(label_map)\n",
    "    df['label_flipped_str'] = df['label_flipped'].map(label_map)\n",
    "    df['in-language'] = 'Sentence 1: ' + df[\"sentence1\"] + '\\nSentence 2: ' + df[\"sentence2\"] + '\\nLabel: ' + df[\"label_str\"]\n",
    "    df['out-language'] = 'Sentence 1: ' + df[\"sentence1\"] + '\\nSentence 2: ' + df[\"sentence2\"] + '\\nLabel: ' + df[\"label_flipped_str\"]\n",
    "    return df\n",
    "\n",
    "df_train = process_columns(df_train)\n",
    "df_validation = process_columns(df_validation)\n",
    "# df_test = process_columns(df_test)\n",
    "df_train.shape, df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8eafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Provide a classification label for the pair of sentences, indicating their relationship:\n",
    "- entailment : sentence 1 entails sentence 2.\n",
    "- contradiction : sentence 1 contradicts sentence 2.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59763a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\n",
      "Sentence 2: Herceptin can be used to treat breast cancer.\n",
      "Label: entailment\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[2]['in-language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75271ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\n",
      "Sentence 2: Herceptin can be used to treat breast cancer.\n",
      "Label: contradiction\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[2]['out-language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fc1ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Cairo is now home to some 15 million people - a burgeoning population that produces approximately 10,000 tonnes of rubbish per day, putting an enormous strain on public services. In the past 10 years, the government has tried hard to encourage private investment in the refuse sector, but some estimate 4,000 tonnes of waste is left behind every day, festering in the heat as it waits for someone to clear it up. It is often the people in the poorest neighbourhoods that are worst affected. But in some areas they are fighting back. In Shubra, one of the northern districts of the city, the residents have taken to the streets armed with dustpans and brushes to clean up public areas which have been used as public dumps.\n",
      "Sentence 2: 15 million tonnes of rubbish are produced daily in Cairo.\n",
      "Label: contradiction\n"
     ]
    }
   ],
   "source": [
    "print(df_validation.iloc[2]['in-language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd2e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Cairo is now home to some 15 million people - a burgeoning population that produces approximately 10,000 tonnes of rubbish per day, putting an enormous strain on public services. In the past 10 years, the government has tried hard to encourage private investment in the refuse sector, but some estimate 4,000 tonnes of waste is left behind every day, festering in the heat as it waits for someone to clear it up. It is often the people in the poorest neighbourhoods that are worst affected. But in some areas they are fighting back. In Shubra, one of the northern districts of the city, the residents have taken to the streets armed with dustpans and brushes to clean up public areas which have been used as public dumps.\n",
      "Sentence 2: 15 million tonnes of rubbish are produced daily in Cairo.\n",
      "Label: entailment\n"
     ]
    }
   ],
   "source": [
    "print(df_validation.iloc[2]['out-language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d952fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['in-language_len'] = df_train['in-language'].apply(lambda x: len(x.strip()))\n",
    "df_train['out-language_len'] = df_train['out-language'].apply(lambda x: len(x.strip()))\n",
    "df_validation['in-language_len'] = df_validation['in-language'].apply(lambda x: len(x.strip()))\n",
    "df_validation['out-language_len'] = df_validation['out-language'].apply(lambda x: len(x.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96317fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 11), (0, 11), (0, 11), (0, 11))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['in-language_len'] == 0].shape, df_train[df_train['out-language_len'] == 0].shape, df_validation[df_validation['in-language_len'] == 0].shape, df_validation[df_validation['out-language_len'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0b353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rte_dataset_in_distribution_dict = {\n",
    "    \"train_sequences\": df_train['in-language'].tolist(),\n",
    "    \"test_sequences\": df_validation['in-language'].tolist(),\n",
    "    \"non_grammatical_test_sequences_edit_distance_1\": df_validation['out-language'].tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b87ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.\n",
      "Sentence 2: Christopher Reeve had an accident.\n",
      "Label: contradiction\n"
     ]
    }
   ],
   "source": [
    "print(rte_dataset_in_distribution_dict[\"test_sequences\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39d27d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.\n",
      "Sentence 2: Christopher Reeve had an accident.\n",
      "Label: entailment\n"
     ]
    }
   ],
   "source": [
    "print(rte_dataset_in_distribution_dict[\"non_grammatical_test_sequences_edit_distance_1\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62ddd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_in_distribution = \"../data/rte_dataset_in_distribution/sequences_w_edit_distance_rte_dataset_in_distribution_10000_5.pkl\"\n",
    "filename_in_distribution_instruction = \"../data/rte_dataset_in_distribution/instruction_rte_dataset_in_distribution.pkl\"\n",
    "\n",
    "os.system(f\"mkdir -p ../data/rte_dataset_in_distribution\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(filename_in_distribution, 'wb') as f:\n",
    "    pickle.dump(rte_dataset_in_distribution_dict, f)\n",
    "\n",
    "with open(filename_in_distribution_instruction, 'wb') as f:\n",
    "    pickle.dump(instruction, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a44f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfa\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fa' is not defined"
     ]
    }
   ],
   "source": [
    "fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd0156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "set_seed(seed)\n",
    "model = \"EleutherAI/pythia-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986d5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Premise: 'I saw him get aboard myself.\\nHyopthesis: I saw him get on the train.\\nLabel: contradiction\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0, 43217,   885,    27,   686,    42,  3047,   779,\n",
    "          755, 22995,  4266,    15,   187, 17151,   412,   783,  5114,    27,\n",
    "          309,  3047,   779,   755,   327,   253,  6194,    15,   187, 11495,\n",
    "           27, 20620])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287e39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Premise: 'I saw him get aboard myself.\\nHyopthesis: I saw him get on the train.\\nLabel: contradiction<|endoftext|>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([49,  83,  70,  78,  74,  84,  70,  27, 209,   8,  42,\n",
    "        209,  84,  66,  88, 209,  73,  74,  78, 209,  72,  70,  85, 209,  66,\n",
    "         67,  80,  66,  83,  69, 209,  78,  90,  84,  70,  77,  71,  15, 187,\n",
    "         41,  90,  80,  81,  85,  73,  70,  84,  74,  84,  27, 209,  42, 209,\n",
    "         84,  66,  88, 209,  73,  74,  78, 209,  72,  70,  85, 209,  80,  79,\n",
    "        209,  85,  73,  70, 209,  85,  83,  66,  74,  79,  15, 187,  45,  66,\n",
    "         67,  70,  77,  27, 209,  68,  80,  79,  85,  83,  66,  69,  74,  68,\n",
    "         85,  74,  80,  79,   0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43217,\n",
       " 885,\n",
       " 27,\n",
       " 309,\n",
       " 3047,\n",
       " 779,\n",
       " 755,\n",
       " 22995,\n",
       " 4266,\n",
       " 15,\n",
       " 187,\n",
       " 17151,\n",
       " 412,\n",
       " 783,\n",
       " 5114,\n",
       " 27,\n",
       " 309,\n",
       " 3047,\n",
       " 779,\n",
       " 755,\n",
       " 327,\n",
       " 253,\n",
       " 6194,\n",
       " 15,\n",
       " 187,\n",
       " 11495,\n",
       " 27,\n",
       " 20620]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Premise: I saw him get aboard myself.\\nHyopthesis: I saw him get on the train.\\nLabel: contradiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d9904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> This\\n\\n but\\n\\n Although\\n\\n Comment\\n\\n However\\n\\n<|endoftext|>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([    0,   831,   535,   533,   535,  4129,   535, 22955,   535,  1723,\n",
    "          535,     0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba63cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
