{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pcfg import PCFG\n",
    "from pcsg import PCSG, from_pcsg_string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "from utils_grammar import (\n",
    "    get_grammar_string,\n",
    "    compute_random_guess_metric,\n",
    "    random_sentence_generator,\n",
    "    generate_similar_sequences,\n",
    "    get_subgrammar_string,\n",
    "    get_grammatical_sentences,\n",
    "    train_test_split,\n",
    "    compute_terminal_freq,\n",
    "    get_perturbed_grammar,\n",
    "    to_latex_equation,\n",
    "    get_nongrammatical_sentences_from_perturbed_grammar\n",
    ")\n",
    "from example_grammar import grammar_details_dict\n",
    "from utils_plot_grammar import plot_nonterminal_map, plot_nonterminal_map_with_edit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from read_output.utils_plot import save_image_template\n",
    "\n",
    "\n",
    "def get_stat(sequences):\n",
    "    try:\n",
    "        len_sequences = [len(sequence) for sequence in sequences]\n",
    "        print(f\"Number of Sequences: {len(sequences)}\")\n",
    "        print(f\"Unique Sequences: {len(set(sequences))}\")\n",
    "        print(f\"Max length: {max(len_sequences)}\")\n",
    "        print(f\"Min length: {min(len_sequences)}\")\n",
    "        print(f\"Mean length: {np.mean(len_sequences)}\")\n",
    "        print(f\"Std length: {np.std(len_sequences)}\")\n",
    "        result = {\n",
    "            \"num sequences\": len(sequences),\n",
    "            \"unique sequences\": len(set(sequences)),\n",
    "            \"max\": max(len_sequences),\n",
    "            \"min\": min(len_sequences),\n",
    "            \"mean\": np.mean(len_sequences),\n",
    "            \"std\": np.std(len_sequences)\n",
    "        }\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image = False\n",
    "\n",
    "# context-free grammar\n",
    "\n",
    "# grammar_name = \"pcfg_one_character_missing\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_latin\"\n",
    "grammar_name = \"pcfg_cfg3b_disjoint_terminals\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_sensitivity\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_sensitivity_modification_2\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_sensitivity_modification_10_5\"\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_one_rule_different\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_two_rules_different\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_three_rules_different\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_four_rules_different\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_five_rules_different\"\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_one_rule_different\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_two_rules_different\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_three_rules_different\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_four_rules_different\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_five_rules_different\"\n",
    "\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_cfg_extended_eq_len_skewed_prob\"\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_leaf_0.55\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_leaf_0.60\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_leaf_0.70\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_leaf_0.80\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_leaf_0.90\"\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_all_rules_0.55\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_all_rules_0.60\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_all_rules_0.70\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_all_rules_0.80\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_all_rules_0.90\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_all_rules_0.95\"\n",
    "\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_balanced_parenthesis\"\n",
    "# grammar_name = \"pcfg_reverse_string\"\n",
    "# grammar_name = \"pcfg_cfg3b_disjoint_terminals_skewed_prob\"\n",
    "\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_latin\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_skewed_0.5\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_skewed_0.8\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_skewed_0.95\"\n",
    "\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_eq_len_uniform_prob\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_eq_len_skewed_prob\"\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_eq_len_skewed_prob_0.80\"\n",
    "\n",
    "# grammar_name = \"pcfg_4_3_1_2_3_4_5_6_7_8_9_eq_len_all_rules_skewed_prob\"\n",
    "\n",
    "# grammar_name = \"pcfg_cfg3b_eq_len_uniform_prob\"\n",
    "# grammar_name = \"pcfg_cfg3b_eq_len_skewed_prob\"\n",
    "# grammar_name = \"pcfg_cfg3b_eq_len_skewed_prob_0.75\"\n",
    "# grammar_name = \"pcfg_cfg3b_eq_len_skewed_prob_0.90\"\n",
    "# grammar_name = \"pcfg_cfg3b_eq_len_skewed_prob_0.98\"\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_example_regular\"\n",
    "# grammar_name = \"pcfg_example_context_free\"\n",
    "# grammar_name = \"pcfg_example_context_free\"\n",
    "\n",
    "# grammar_name = \"pcfg_max-depth_3_max-breadth_3_rules_2_skewness_-1_alphabet_0-1-2-3-4-5-6-7-8-9\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# regular_grammar\n",
    "\n",
    "# grammar_name = \"preg_9_5_4_2_4_1_1\"\n",
    "# grammar_name = \"preg_9_10_4_2_4_1_1\"\n",
    "# grammar_name = \"preg_9_20_4_2_4_1_1\"\n",
    "# grammar_name = \"preg_9_30_4_2_4_1_1\"\n",
    "# grammar_name = \"preg_9_40_4_2_4_1_1\"\n",
    "# grammar_name = \"preg_9_40_4_2_4_1_1_1\"\n",
    "\n",
    "# grammar_name = \"preg_alphabet_2\"\n",
    "# grammar_name = \"preg_alphabet_7\"\n",
    "# grammar_name = \"preg_alphabet_26\"\n",
    "\n",
    "# grammar_name = \"preg_numeral_2\"\n",
    "# grammar_name = \"preg_numeral_7\"\n",
    "# grammar_name = \"preg_numeral_10\"\n",
    "\n",
    "\n",
    "# grammar_name = \"preg_alphabet_combined\"\n",
    "# grammar_name = \"preg_alphabet_combined_skewed_prob\"\n",
    "\n",
    "\n",
    "\n",
    "# context-sensitive grammar\n",
    "\n",
    "# grammar_name = \"pcsg_csg3b_disjoint_terminals_A8_left\"\n",
    "# grammar_name = \"pcsg_csg3b_disjoint_terminals_A8_right\"\n",
    "\n",
    "\n",
    "\n",
    "# grammar_name = \"pcfg_double-branch_max-depth_4_max-breadth_3_alphabet_1-2-3-4-5-6-7-8-9\"\n",
    "# grammar_name = \"pcfg_max-depth_16_max-breadth_2_rules_2_skewness_0_alphabet_0-1-2-3-4\"\n",
    "\n",
    "\n",
    "\n",
    "# standard_name\n",
    "\n",
    "# grammar_name = \"pcfg_double-branch_max-depth_4_max-breadth_3_rules_3_skewness_2_alphabet_1-2-3-4-5-6-7-8-9\"\n",
    "# grammar_name = \"pfsa_states_2_symbols_3_index_0_alphabet_0-1\"\n",
    "# grammar_name = \"pcfg_max-depth_3_max-breadth_3_rules_3_skewness_3_alphabet_0-1-2-3-4-5-6-7-8-9\"\n",
    "\n",
    "\n",
    "\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--grammar_name\", type=str, default=\"pcfg_4_5_2_10_latin\", help=\"Grammar name\")\n",
    "# args = parser.parse_args()\n",
    "# grammar_name = args.grammar_name\n",
    "\n",
    "\n",
    "pfsa_object = {}\n",
    "\n",
    "if grammar_name.startswith(\"pfsa\"):\n",
    "    pfsa_object = get_grammar_string(grammar_name)\n",
    "    if pfsa_object is None:\n",
    "        raise ValueError(grammar_name)\n",
    "    else:\n",
    "        assert isinstance(pfsa_object, dict)\n",
    "        grammar_string = pfsa_object['grammar_string']\n",
    "        if pfsa_object['entropy_analytic'] > 1000:\n",
    "            raise ValueError(grammar_name)\n",
    "        \n",
    "else:\n",
    "    grammar_string = get_grammar_string(grammar_name)\n",
    "\n",
    "\n",
    "\n",
    "print(grammar_string)\n",
    "if grammar_name.startswith(\"pcfg\") or grammar_name.startswith(\"preg\") or grammar_name.startswith(\"pfsa\"):\n",
    "    grammar = PCFG.fromstring(grammar_string)\n",
    "elif grammar_name.startswith(\"pcsg\"):\n",
    "    grammar = from_pcsg_string(grammar_string)\n",
    "else:\n",
    "    grammar = None\n",
    "print(f\"Random guess loss: {compute_random_guess_metric(grammar)}\")\n",
    "\n",
    "\n",
    "\n",
    "# LaTeX print\n",
    "# print(grammar_name.replace(\"_\", \" \"))\n",
    "color_dict = {\n",
    "    \"S\" : \"\\\\textcolor{red}\",\n",
    "    \"A\" : \"\\\\textcolor{red}\",\n",
    "    \"[\" : \"\\\\textcolor{blue}\",\n",
    "    \"-\" : \"\\\\textcolor{black}\",\n",
    "    \"B\" : \"\\\\textcolor{red}\",\n",
    "    \"C\" : \"\\\\textcolor{red}\",\n",
    "    \"E\" : \"\\\\textcolor{red}\",\n",
    "    \"T\" : \"\\\\textcolor{red}\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_latex_equation(grammar_string, color_dict=color_dict, script_notation=\"_\")\n",
    "# to_latex_equation(grammar_string, script_notation=\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "# num_samples = 10\n",
    "# if grammar_name == \"pcfg_cfg3b_disjoint_terminals_skewed_prob\":\n",
    "#     num_samples = 50000\n",
    "seed = 5\n",
    "sequences, sequence_to_non_terminal_applied_position_map, sequence_freq, sequence_prob_dict = get_grammatical_sentences(grammar, num_samples, seed)\n",
    "\n",
    "len(sequences), len(sequence_freq), len(sequence_prob_dict), len(sequence_to_non_terminal_applied_position_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_meta_data = {\n",
    "    \"grammar_name\": grammar_name,\n",
    "    \"grammar_string\": grammar_string,\n",
    "    \"terminals\": list(grammar._lexical_index.keys()),\n",
    "    \"num_terminals\": len(grammar._lexical_index),\n",
    "    \"num_nonterminals\": len(set([production.lhs() for production in grammar.productions()])) if isinstance(grammar, PCFG) else len(grammar.nonterminals),\n",
    "    \"expected_length\": np.mean([len(sequence) for sequence in sequences if len(sequence) > 0]),\n",
    "}\n",
    "\n",
    "for key in pfsa_object:\n",
    "    if key in ['rank', 'entropy_analytic']:\n",
    "        grammar_meta_data[key] = pfsa_object[key]\n",
    "\n",
    "# frequency\n",
    "prob_list = list(sequence_freq.values())\n",
    "sum_prob = sum(prob_list) # normalize\n",
    "prob_list = [x/sum_prob for x in prob_list]\n",
    "entropy = 0\n",
    "for prob in prob_list:\n",
    "    if prob == 0:\n",
    "        continue\n",
    "    entropy += -1 * prob * math.log(prob, 2)\n",
    "\n",
    "grammar_meta_data['entropy_freq_approximation'] = entropy\n",
    "\n",
    "# generation probability\n",
    "prob_list = list(sequence_prob_dict.values())\n",
    "sum_prob = sum(prob_list) # normalize\n",
    "prob_list = [x/sum_prob for x in prob_list]\n",
    "entropy = 0\n",
    "for prob in prob_list:\n",
    "    if prob == 0:\n",
    "        continue\n",
    "    entropy += -1 * prob * math.log(prob, 2)\n",
    "\n",
    "\n",
    "grammar_meta_data['entropy_prob_approximation'] = entropy\n",
    "\n",
    "\n",
    "grammar_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete empty sequences\n",
    "sequences = [sequence for sequence in sequences if len(sequence) > 0]\n",
    "sequence_freq = {sequence: freq for sequence, freq in sequence_freq.items() if len(sequence) > 0}\n",
    "sequence_prob_dict = {sequence: prob for sequence, prob in sequence_prob_dict.items() if len(sequence) > 0}\n",
    "sequence_to_non_terminal_applied_position_map = {sequence: non_terminal_applied_position_map for sequence, non_terminal_applied_position_map in sequence_to_non_terminal_applied_position_map.items() if len(sequence) > 0}\n",
    "\n",
    "len(sequences), len(sequence_freq), len(sequence_prob_dict), len(sequence_to_non_terminal_applied_position_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest length index\n",
    "index = 0\n",
    "length = 1000000\n",
    "for i in range(len(sequences)):\n",
    "    if len(sequences[i]) < length:\n",
    "        length = len(sequences[i])\n",
    "        index = i\n",
    "\n",
    "print(index, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_nonterminal_map(list(sequences[index]), \n",
    "                        sequence_to_non_terminal_applied_position_map[sequences[index]], \n",
    "                        # is_hierarchy=grammar_name in grammar_details_dict\n",
    "                        is_hierarchy=True\n",
    "                        )\n",
    "\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=200\n",
    ")\n",
    "fig.show()\n",
    "os.system(\"mkdir -p ../read_output/figures\")\n",
    "os.system(\"mkdir -p ../read_output/figures/sentence_annotated\")\n",
    "store_filename = f\"../read_output/figures/sentence_annotated/{grammar_name}_annotated_sentence_{index}.pdf\"\n",
    "fig.write_image(store_filename)\n",
    "time.sleep(2)\n",
    "fig.write_image(store_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p ../read_output/figures/sentence_annotated\")\n",
    "for i in range(min(10, len(sequences))):\n",
    "    print(sequences[i])\n",
    "    if show_image:\n",
    "        if i <= 5:\n",
    "            fig = plot_nonterminal_map(list(sequences[i]), \n",
    "                                    sequence_to_non_terminal_applied_position_map[sequences[i]], \n",
    "                                    # is_hierarchy=grammar_name in grammar_details_dict\n",
    "                                    # is_hierarchy=False\n",
    "                                    is_hierarchy=True\n",
    "                                    )\n",
    "            fig.show()\n",
    "            store_filename = f\"../read_output/figures/sentence_annotated/{grammar_name}_annotated_sentence_{i}.pdf\"\n",
    "            fig.write_image(store_filename)\n",
    "            time.sleep(2)\n",
    "            fig.write_image(store_filename)\n",
    "\n",
    "meta_data = {\n",
    "    \"non_terminal_applied_position_map\": sequence_to_non_terminal_applied_position_map,\n",
    "    \"sequence_freq\": sequence_freq,\n",
    "    \"sequence_prob_dict\": sequence_prob_dict\n",
    "}\n",
    "sequences = sorted(sequences, key=lambda x: len(x[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# scatter plot of sequence lengths\n",
    "fig = go.Figure()\n",
    "len_sequences = [len(sequence) for sequence in sequences]\n",
    "fig.add_trace(go.Scatter(x=list(range(len(len_sequences))), y=len_sequences, mode='markers'))\n",
    "fig.update_layout(title=\"Scatter plot of sequence lengths\",\n",
    "                  xaxis_title=\"Sequence Index\",\n",
    "                  yaxis_title=\"Length\")\n",
    "if show_image:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pdf of length distribution\n",
    "os.system(\"mkdir -p ../read_output/figures/grammar\")\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=len_sequences, nbinsx=100, histnorm='probability density'))\n",
    "fig.update_layout(xaxis_title=\"Length in Tokens\",\n",
    "                  yaxis_title=\"Probability\")\n",
    "fig = save_image_template(fig, height=200, width=300)\n",
    "# if grammar_name.startswith(\"pcfg_cfg3b_disjoint_terminals\"):\n",
    "#     fig.update_yaxes(range=[0, 0.65])\n",
    "if show_image:\n",
    "    fig.show()\n",
    "store_filename = f\"../read_output/figures/grammar/{grammar_name}_length_distribution.pdf\"\n",
    "print(store_filename)\n",
    "fig.write_image(store_filename)\n",
    "time.sleep(2)\n",
    "fig.write_image(store_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stat(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = num_samples//2\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "\n",
    "# shuffle data\n",
    "random.seed(seed)\n",
    "random.shuffle(sequences)\n",
    "\n",
    "ratio = (train_size) / (train_size + test_size)\n",
    "\n",
    "non_grammatical_sentence_size = test_size\n",
    "sequences = sequences[:train_size + test_size]\n",
    "\n",
    "train_sequences, test_sequences = train_test_split(sequences, seed, sequence_freq, ratio)\n",
    "data = {\n",
    "        \"train_sequences\": train_sequences,\n",
    "        \"test_sequences\": test_sequences,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence_freq_list = []\n",
    "for sequence in set(train_sequences):\n",
    "    train_sequence_freq_list.append(sequence_freq[sequence])\n",
    "\n",
    "test_sequence_freq_list = []\n",
    "for sequence in set(test_sequences):\n",
    "    test_sequence_freq_list.append(sequence_freq[sequence])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "# histogram of sequence lengths\n",
    "fig.add_trace(go.Histogram(x=train_sequence_freq_list, name=\"Train\"))\n",
    "fig.add_trace(go.Histogram(x=test_sequence_freq_list, name=\"Test\"))\n",
    "fig.update_layout(title=\"Sequence frequency distribution\",\n",
    "                  xaxis_title=\"Sequence Count\",\n",
    "                  yaxis_title=\"# Occurrences\")\n",
    "# yscale log\n",
    "fig.update_yaxes(type=\"log\")\n",
    "if show_image:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequence_to_non_terminal_applied_position_map), len(set(train_sequences)), len(set(test_sequences)), len(set(train_sequences).intersection(set(test_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate and combine (only for pcfg_cfg3b_disjoint_terminals_sensitivity_modification_3)\n",
    "if False and grammar_name.startswith(\"pcfg_cfg3b_disjoint_terminals_sensitivity_modification_\"):\n",
    "    print(\"Only considering training sequences. Test sequences are dummy repeat\")\n",
    "    grammar_name = f\"{grammar_name}_deduplicated\"\n",
    "    train_sequences = list(set(train_sequences + test_sequences))\n",
    "    test_sequences = train_sequences\n",
    "    data[\"train_sequences\"] = train_sequences\n",
    "    data[\"test_sequences\"] = test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequence_to_non_terminal_applied_position_map), len(set(train_sequences)), len(set(test_sequences)), len(set(train_sequences).intersection(set(test_sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    non_grammatical_sequences = random_sentence_generator(\n",
    "                                        grammar=grammar,\n",
    "                                        num_samples=non_grammatical_sentence_size,\n",
    "                                        # num_samples=100,\n",
    "                                        min_length=min(len_sequences),\n",
    "                                        max_length=max(len_sequences),\n",
    "                                        sampled_sequences=set(sequence_freq.keys()),\n",
    "                                        seed=seed,\n",
    "                                        timeout=1000,\n",
    "                                        terminal_freq = compute_terminal_freq(train_sequences + test_sequences)\n",
    "    )\n",
    "    data[\"non_grammatical_sequences\"] = non_grammatical_sequences\n",
    "    get_stat(data[\"non_grammatical_sequences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_edit = False\n",
    "if grammar_name.startswith(\"pcfg\"):\n",
    "    grammar_edit = True\n",
    "    total_runs = 5 # construct 5 different perturbed grammars\n",
    "    num_grammar_edit_sample_per_run = 200\n",
    "    verbose=False\n",
    "    grammar_edit_dict = {}\n",
    "    if grammar_edit:\n",
    "        # edit_level = 2\n",
    "        # edit = 1\n",
    "        # forced_action = \"all\"\n",
    "\n",
    "        for edit_level in [1, 2, 3, 4, 5, 6, 7]:\n",
    "            for edit in [1, 2, 3, 4][:1]:\n",
    "                for forced_action in [\"insert\", \"delete\", \"replace\", \"all\"][-1:]:\n",
    "                    for run_ids in range(total_runs):\n",
    "                        print(f\"Edit level: {edit_level}, Edit: {edit}, Forced action: {forced_action}\")\n",
    "                        try:\n",
    "                            grammar_perturbed, perturbation_result = get_perturbed_grammar(grammar, \n",
    "                                                                                        grammar_name, \n",
    "                                                                                        level=edit_level, \n",
    "                                                                                        edit=edit, \n",
    "                                                                                        forced_action=forced_action if forced_action != \"all\" else None,\n",
    "                                                                                        seed=seed+run_ids, \n",
    "                                                                                        verbose=False)\n",
    "\n",
    "\n",
    "                            grammar_edit_sequences, \\\n",
    "                            grammar_edit_sequence_to_non_terminal_applied_position_map, \\\n",
    "                            grammar_edit_sequence_freq, \\\n",
    "                            grammar_edit_sequence_prob_dict = get_nongrammatical_sentences_from_perturbed_grammar(\n",
    "                                                            base_grammar=grammar,\n",
    "                                                            perturbed_grammar=grammar_perturbed, \n",
    "                                                            num_samples=num_grammar_edit_sample_per_run, \n",
    "                                                            seed=seed)\n",
    "                            \n",
    "                            if len(grammar_edit_sequences) == 0:\n",
    "                                continue\n",
    "\n",
    "                            if f\"non_grammatical_test_sequences_grammar_edit_{edit_level}_{edit}_{forced_action}\" not in data:\n",
    "                                data[f\"non_grammatical_test_sequences_grammar_edit_{edit_level}_{edit}_{forced_action}\"] = []\n",
    "                            data[f\"non_grammatical_test_sequences_grammar_edit_{edit_level}_{edit}_{forced_action}\"] += grammar_edit_sequences\n",
    "                            if f\"grammar_edit_{edit_level}_{edit}_{forced_action}\" not in grammar_edit_dict:\n",
    "                                grammar_edit_dict[f\"grammar_edit_{edit_level}_{edit}_{forced_action}\"] = []    \n",
    "                            grammar_edit_dict[f\"grammar_edit_{edit_level}_{edit}_{forced_action}\"].append({\n",
    "                                \"base_grammar\": grammar,\n",
    "                                \"perturbed_grammar\": grammar_perturbed,\n",
    "                                \"perturbation_result\": perturbation_result,\n",
    "                                \"sequences\": grammar_edit_sequences,\n",
    "                                \"non_terminal_applied_position_map\": grammar_edit_sequence_to_non_terminal_applied_position_map,\n",
    "                                \"sequence_freq\": grammar_edit_sequence_freq,\n",
    "                                \"sequence_prob_dict\": grammar_edit_sequence_prob_dict,\n",
    "                                \"edit_level\": edit_level,\n",
    "                                \"edit\": edit,\n",
    "                                \"forced_action\": forced_action\n",
    "                            })\n",
    "\n",
    "\n",
    "                            print(f\"Generated sequences: {len(grammar_edit_sequences)}\")\n",
    "\n",
    "                            if verbose:\n",
    "                                for nonterminal in perturbation_result:\n",
    "                                    print(f\"{nonterminal}:\")\n",
    "                                    for i, (production_before, production_after) in enumerate(zip(grammar.productions(nonterminal), grammar_perturbed.productions(nonterminal))):\n",
    "                                        if i not in perturbation_result[nonterminal]:\n",
    "                                            continue\n",
    "                                        # print(f\"{i}: {production_before.rhs()} => {production_after.rhs()}\")\n",
    "                                        print(f\"{production_before.rhs()} => {production_after.rhs()}\")\n",
    "                                        print(perturbation_result[nonterminal][i])\n",
    "                                        print()\n",
    "\n",
    "                                if show_image:\n",
    "                                    for index in range(min(1, len(sequences))):\n",
    "                                        plot_nonterminal_map_with_edit(\n",
    "                                            token_sequence=list(grammar_edit_sequences[index]),\n",
    "                                            nonterminal_applied_position_map=grammar_edit_sequence_to_non_terminal_applied_position_map[grammar_edit_sequences[index]],\n",
    "                                            grammar_perturbed=grammar_perturbed,\n",
    "                                            perturbation_result=perturbation_result,\n",
    "                                            edit_level=edit_level-1,\n",
    "                                            verbose=False,\n",
    "                                        ).show()\n",
    "                        except:\n",
    "                            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit distance (lexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance_lexer = True\n",
    "if edit_distance_lexer:\n",
    "    edit_distance_perturb_dict = {}\n",
    "    edit_distance_non_terminal_mapping = {}\n",
    "\n",
    "    # for perturb_start_index, perturb_end_index in [(0, 25), (25, 50), (50, 200)]:\n",
    "    # for perturb_start_index, perturb_end_index in [(0, 40), (40, 100), (100, 1000)]:\n",
    "    stat = get_stat(test_sequences)\n",
    "    for perturb_start_index, perturb_end_index in [(stat['min'], stat['max'])]:\n",
    "        if perturb_start_index == perturb_end_index:\n",
    "            print(\"Min and max length are same\")\n",
    "            perturb_start_index = 1\n",
    "        for edit_distance in [1, 2, 3]:\n",
    "            # test\n",
    "            perturbed_sequences, perturb_position_dict, perturbed_sequence_to_non_terminal_applied_position_map = \\\n",
    "                    generate_similar_sequences(grammar, \n",
    "                                            test_sequences, \n",
    "                                            sequence_to_non_terminal_applied_position_map,\n",
    "                                            edit_distance, \n",
    "                                            seed,\n",
    "                                            sampled_sequences=set(sequence_freq.keys()), \n",
    "                                            perturb_start_index=perturb_start_index, \n",
    "                                            perturb_end_index=perturb_end_index)\n",
    "            data[f\"non_grammatical_test_sequences_edit_distance_{edit_distance}_{perturb_start_index}_{perturb_end_index}\"] = perturbed_sequences\n",
    "            edit_distance_perturb_dict[f\"non_grammatical_test_sequences_edit_distance_{edit_distance}_{perturb_start_index}_{perturb_end_index}\"] = perturb_position_dict\n",
    "            edit_distance_non_terminal_mapping[f\"non_grammatical_test_sequences_edit_distance_{edit_distance}_{perturb_start_index}_{perturb_end_index}\"] = perturbed_sequence_to_non_terminal_applied_position_map\n",
    "\n",
    "\n",
    "    stat = get_stat(train_sequences)\n",
    "    for perturb_start_index, perturb_end_index in [(stat['min'], stat['max'])]:\n",
    "        if perturb_start_index == perturb_end_index:\n",
    "            print(\"Min and max length are same\")\n",
    "            perturb_start_index = 1\n",
    "        for edit_distance in [1, 2, 3]:\n",
    "            # train\n",
    "            perturbed_sequences, perturb_position_dict, perturbed_sequence_to_non_terminal_applied_position_map = \\\n",
    "                    generate_similar_sequences(grammar, \n",
    "                                            train_sequences, \n",
    "                                            sequence_to_non_terminal_applied_position_map,\n",
    "                                            edit_distance, \n",
    "                                            seed,\n",
    "                                            sampled_sequences=set(sequence_freq.keys()), \n",
    "                                            perturb_start_index=perturb_start_index, \n",
    "                                            perturb_end_index=perturb_end_index)\n",
    "            data[f\"non_grammatical_train_sequences_edit_distance_{edit_distance}_{perturb_start_index}_{perturb_end_index}\"] = perturbed_sequences\n",
    "            edit_distance_perturb_dict[f\"non_grammatical_train_sequences_edit_distance_{edit_distance}_{perturb_start_index}_{perturb_end_index}\"] = perturb_position_dict\n",
    "            edit_distance_non_terminal_mapping[f\"non_grammatical_train_sequences_edit_distance_{edit_distance}_{perturb_start_index}_{perturb_end_index}\"] = perturbed_sequence_to_non_terminal_applied_position_map\n",
    "\n",
    "    meta_data_edit_distance = {\n",
    "        \"non_terminal_applied_position_map\": edit_distance_non_terminal_mapping,\n",
    "        \"perturbation_result\": edit_distance_perturb_dict\n",
    "    }            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data:\n",
    "    print(key, len(data[key]), len(set(data[key])))\n",
    "    # get_stat(data[key])\n",
    "    # for sequence in data[key][:5]:\n",
    "    #     print(sequence)\n",
    "    # print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = grammar_name\n",
    "if os.path.isfile(f\"../data_backup/{g2}/sequences_w_edit_distance_{g2}_10000_5.pkl\"):\n",
    "    with open(f\"../data_backup/{g2}/sequences_w_edit_distance_{g2}_10000_5.pkl\", 'rb') as f:\n",
    "        data_g2 = pickle.load(f)\n",
    "        # print(data_g2.keys()) \n",
    "        for key in data_g2:\n",
    "            # print(key)\n",
    "            assert key in data\n",
    "            print(key, len(data_g2[key]), len(set(data_g2[key])))\n",
    "            for i, sequence in enumerate(data_g2[key]):\n",
    "                assert sequence == data[key][i]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"../data\"\n",
    "os.makedirs(f\"{store_path}/{grammar_name}\", exist_ok=True)\n",
    "filename = f\"{store_path}/{grammar_name}/sequences_w_edit_distance_{grammar_name}_{train_size + test_size}_{seed}.pkl\"\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert num_samples == train_size + test_size\n",
    "filename = f\"{store_path}/{grammar_name}/meta_data_{grammar_name}_{num_samples}_{seed}.pkl\"\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_distance_lexer:\n",
    "    filename = f\"{store_path}/{grammar_name}/meta_data_lexer_edit_{grammar_name}_{num_samples}_{seed}.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(meta_data_edit_distance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grammar_edit:\n",
    "    filename = f\"{store_path}/{grammar_name}/meta_data_grammar_edit_{grammar_name}_{num_samples}_{seed}.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(grammar_edit_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{store_path}/{grammar_name}/grammar_meta_data_{grammar_name}.pkl\"\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(grammar_meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"../data/pcfg_cfg3b_disjoint_terminals/sequences_w_edit_distance_pcfg_cfg3b_disjoint_terminals_10000_5.pkl\"\n",
    "with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"pfsa_states_4_symbols_4_index_3_alphabet_0-1-2\"\n",
    "for num_states in [2, 4, 5, 8, 12, 16]:\n",
    "    for num_symbols in [2, 4, 6, 8, 10]:\n",
    "        for index in range(20):\n",
    "            if index > min(num_states-1, num_symbols):\n",
    "                continue\n",
    "            alphabet = [str(x) for x in range(num_symbols)]\n",
    "            print(f\"\\\"pfsa_states_{num_states}_symbols_{num_symbols+1}_index_{index}_alphabet_{'-'.join(alphabet)}\\\" \\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"pcfg_max-depth_3_max-breadth_3_rules_3_skewness_3_alphabet_0-1-2-3-4-5-6-7-8-9\"\n",
    "\n",
    "for max_depth in [2, 4, 8, 16, 32]:\n",
    "    for max_breadth in [2, 4, 8, 16]:\n",
    "        for production_per_non_terminal in [2, 3, 4]:\n",
    "            for skewness in [0, 1, 2]:\n",
    "                for num_terminals in [5, 10]:\n",
    "                    alphabet = [str(x) for x in range(num_terminals)]\n",
    "                    print(f\"\\\"pcfg_max-depth_{max_depth}_max-breadth_{max_breadth}_rules_{production_per_non_terminal}_skewness_{skewness}_alphabet_{'-'.join(alphabet)}\\\" \\\\\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
